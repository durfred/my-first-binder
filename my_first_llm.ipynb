{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOnmwPfEZYHqvE/uNxTH4ai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durfred/my-first-binder/blob/main/my_first_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2ylx7ptcupk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "write a python script to return rss from bbc"
      ],
      "metadata": {
        "id": "bXASkU4kur0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 确保您在 nanoGPT 目录下\n",
        "%cd /content/nanoGPT\n",
        "\n",
        "!python train.py \\\n",
        "    --dataset=shakespeare \\\n",
        "    --n_layer=4 \\\n",
        "    --n_head=4 \\\n",
        "    --n_embd=64 \\\n",
        "    --block_size=64 \\\n",
        "    --batch_size=16 \\\n",
        "    --max_iters=2000 \\\n",
        "    --eval_interval=200 \\\n",
        "    --eval_iters=20 \\\n",
        "    --log_interval=10 \\\n",
        "    --compile=False \\\n",
        "    --device=cuda \\\n",
        "    --out_dir='out-tiny-shakespeare' # 模型的输出目录\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaZNFnMXvTeo",
        "outputId": "fc3c963c-d0f7-4cd4-ec41-109678477151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n",
            "Overriding: dataset = shakespeare\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 16\n",
            "Overriding: max_iters = 2000\n",
            "Overriding: eval_interval = 200\n",
            "Overriding: eval_iters = 20\n",
            "Overriding: log_interval = 10\n",
            "Overriding: compile = False\n",
            "Overriding: device = cuda\n",
            "Overriding: out_dir = out-tiny-shakespeare\n",
            "tokens per iteration will be: 40,960\n",
            "Initializing a new model from scratch\n",
            "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
            "number of parameters: 3.42M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 18, with 3,420,160 parameters\n",
            "num non-decayed parameter tensors: 9, with 576 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 10.8260, val loss 10.8231\n",
            "iter 0: loss 10.8328, time 1752.56ms, mfu -100.00%\n",
            "iter 10: loss 10.8287, time 917.08ms, mfu 0.30%\n",
            "iter 20: loss 10.8021, time 922.26ms, mfu 0.30%\n",
            "iter 30: loss 10.7932, time 929.61ms, mfu 0.30%\n",
            "iter 40: loss 10.7761, time 936.27ms, mfu 0.30%\n",
            "iter 50: loss 10.7411, time 939.19ms, mfu 0.29%\n",
            "iter 60: loss 10.6929, time 945.60ms, mfu 0.29%\n",
            "iter 70: loss 10.6458, time 946.81ms, mfu 0.29%\n",
            "iter 80: loss 10.6110, time 954.99ms, mfu 0.29%\n",
            "iter 90: loss 10.5953, time 962.26ms, mfu 0.29%\n",
            "iter 100: loss 10.5397, time 971.40ms, mfu 0.29%\n",
            "iter 110: loss 10.5109, time 976.19ms, mfu 0.29%\n",
            "iter 120: loss 10.4492, time 975.14ms, mfu 0.29%\n",
            "iter 130: loss 10.4199, time 978.40ms, mfu 0.29%\n",
            "iter 140: loss 10.3502, time 978.72ms, mfu 0.29%\n",
            "iter 150: loss 10.3206, time 972.02ms, mfu 0.29%\n",
            "iter 160: loss 10.2679, time 968.80ms, mfu 0.28%\n",
            "iter 170: loss 10.2466, time 967.21ms, mfu 0.28%\n",
            "iter 180: loss 10.1746, time 964.70ms, mfu 0.28%\n",
            "iter 190: loss 10.1267, time 977.78ms, mfu 0.28%\n",
            "step 200: train loss 10.0520, val loss 10.0571\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 200: loss 10.0404, time 1440.30ms, mfu 0.27%\n",
            "iter 210: loss 9.9870, time 972.73ms, mfu 0.27%\n",
            "iter 220: loss 9.9358, time 971.57ms, mfu 0.28%\n",
            "iter 230: loss 9.8725, time 968.30ms, mfu 0.28%\n",
            "iter 240: loss 9.7882, time 972.91ms, mfu 0.28%\n",
            "iter 250: loss 9.7224, time 966.78ms, mfu 0.28%\n",
            "iter 260: loss 9.6490, time 970.88ms, mfu 0.28%\n",
            "iter 270: loss 9.5400, time 971.41ms, mfu 0.28%\n",
            "iter 280: loss 9.4618, time 974.83ms, mfu 0.28%\n",
            "iter 290: loss 9.3843, time 970.83ms, mfu 0.28%\n",
            "iter 300: loss 9.3047, time 971.09ms, mfu 0.28%\n",
            "iter 310: loss 9.2464, time 968.00ms, mfu 0.28%\n",
            "iter 320: loss 9.1709, time 969.41ms, mfu 0.28%\n",
            "iter 330: loss 9.0635, time 965.93ms, mfu 0.28%\n",
            "iter 340: loss 8.9792, time 975.23ms, mfu 0.28%\n",
            "iter 350: loss 8.8743, time 972.86ms, mfu 0.28%\n",
            "iter 360: loss 8.7722, time 971.35ms, mfu 0.28%\n",
            "iter 370: loss 8.7134, time 970.10ms, mfu 0.28%\n",
            "iter 380: loss 8.5543, time 970.47ms, mfu 0.28%\n",
            "iter 390: loss 8.4531, time 971.17ms, mfu 0.28%\n",
            "step 400: train loss 8.4144, val loss 8.4687\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 400: loss 8.3673, time 1439.47ms, mfu 0.27%\n",
            "iter 410: loss 8.3998, time 967.06ms, mfu 0.27%\n",
            "iter 420: loss 8.1142, time 969.13ms, mfu 0.27%\n",
            "iter 430: loss 8.1968, time 967.69ms, mfu 0.27%\n",
            "iter 440: loss 8.0368, time 974.88ms, mfu 0.27%\n",
            "iter 450: loss 7.9543, time 972.33ms, mfu 0.27%\n",
            "iter 460: loss 7.8871, time 968.92ms, mfu 0.27%\n",
            "iter 470: loss 7.7862, time 968.83ms, mfu 0.28%\n",
            "iter 480: loss 7.6461, time 972.65ms, mfu 0.28%\n",
            "iter 490: loss 7.5348, time 975.47ms, mfu 0.28%\n",
            "iter 500: loss 7.5452, time 968.78ms, mfu 0.28%\n",
            "iter 510: loss 7.5038, time 971.40ms, mfu 0.28%\n",
            "iter 520: loss 7.4274, time 971.32ms, mfu 0.28%\n",
            "iter 530: loss 7.3272, time 970.22ms, mfu 0.28%\n",
            "iter 540: loss 7.1916, time 971.77ms, mfu 0.28%\n",
            "iter 550: loss 7.1816, time 968.93ms, mfu 0.28%\n",
            "iter 560: loss 7.1228, time 967.78ms, mfu 0.28%\n",
            "iter 570: loss 7.0830, time 968.92ms, mfu 0.28%\n",
            "iter 580: loss 6.9163, time 967.00ms, mfu 0.28%\n",
            "iter 590: loss 6.9266, time 966.49ms, mfu 0.28%\n",
            "step 600: train loss 6.7412, val loss 6.8640\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 600: loss 6.6943, time 1441.68ms, mfu 0.27%\n",
            "iter 610: loss 6.5715, time 970.90ms, mfu 0.27%\n",
            "iter 620: loss 6.4562, time 967.29ms, mfu 0.27%\n",
            "iter 630: loss 6.6200, time 968.20ms, mfu 0.27%\n",
            "iter 640: loss 6.5668, time 970.08ms, mfu 0.27%\n",
            "iter 650: loss 6.3709, time 970.33ms, mfu 0.27%\n",
            "iter 660: loss 6.3276, time 973.23ms, mfu 0.27%\n",
            "iter 670: loss 6.3766, time 974.28ms, mfu 0.28%\n",
            "iter 680: loss 6.1944, time 969.15ms, mfu 0.28%\n",
            "iter 690: loss 6.1669, time 966.85ms, mfu 0.28%\n",
            "iter 700: loss 6.2190, time 978.65ms, mfu 0.28%\n",
            "iter 710: loss 5.9646, time 972.83ms, mfu 0.28%\n",
            "iter 720: loss 5.9644, time 972.74ms, mfu 0.28%\n",
            "iter 730: loss 6.1851, time 972.31ms, mfu 0.28%\n",
            "iter 740: loss 6.1165, time 968.28ms, mfu 0.28%\n",
            "iter 750: loss 5.7290, time 973.82ms, mfu 0.28%\n",
            "iter 760: loss 5.9958, time 971.68ms, mfu 0.28%\n",
            "iter 770: loss 5.7945, time 973.28ms, mfu 0.28%\n",
            "iter 780: loss 5.5999, time 971.08ms, mfu 0.28%\n",
            "iter 790: loss 5.6681, time 971.88ms, mfu 0.28%\n",
            "step 800: train loss 5.6948, val loss 5.8899\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 800: loss 5.7175, time 1536.40ms, mfu 0.27%\n",
            "iter 810: loss 5.7657, time 972.86ms, mfu 0.27%\n",
            "iter 820: loss 5.8066, time 974.15ms, mfu 0.27%\n",
            "iter 830: loss 5.5943, time 972.29ms, mfu 0.27%\n",
            "iter 840: loss 5.6067, time 971.73ms, mfu 0.27%\n",
            "iter 850: loss 5.3809, time 973.25ms, mfu 0.27%\n",
            "iter 860: loss 5.4032, time 970.22ms, mfu 0.27%\n",
            "iter 870: loss 5.5859, time 970.81ms, mfu 0.27%\n",
            "iter 880: loss 5.1438, time 973.79ms, mfu 0.27%\n",
            "iter 890: loss 5.2888, time 973.33ms, mfu 0.28%\n",
            "iter 900: loss 5.2450, time 976.97ms, mfu 0.28%\n",
            "iter 910: loss 5.2133, time 972.46ms, mfu 0.28%\n",
            "iter 920: loss 5.3829, time 971.98ms, mfu 0.28%\n",
            "iter 930: loss 5.3075, time 971.99ms, mfu 0.28%\n",
            "iter 940: loss 5.1738, time 962.28ms, mfu 0.28%\n",
            "iter 950: loss 5.0703, time 969.63ms, mfu 0.28%\n",
            "iter 960: loss 5.2746, time 979.77ms, mfu 0.28%\n",
            "iter 970: loss 5.0473, time 971.22ms, mfu 0.28%\n",
            "iter 980: loss 5.0250, time 970.62ms, mfu 0.28%\n",
            "iter 990: loss 5.0411, time 970.07ms, mfu 0.28%\n",
            "step 1000: train loss 4.9906, val loss 5.4250\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 1000: loss 4.9080, time 1461.36ms, mfu 0.27%\n",
            "iter 1010: loss 4.9194, time 969.70ms, mfu 0.27%\n",
            "iter 1020: loss 4.6222, time 972.26ms, mfu 0.27%\n",
            "iter 1030: loss 4.9978, time 972.28ms, mfu 0.27%\n",
            "iter 1040: loss 4.7674, time 972.46ms, mfu 0.27%\n",
            "iter 1050: loss 4.8495, time 974.14ms, mfu 0.27%\n",
            "iter 1060: loss 5.1540, time 975.29ms, mfu 0.27%\n",
            "iter 1070: loss 4.7990, time 972.21ms, mfu 0.27%\n",
            "iter 1080: loss 4.6700, time 972.83ms, mfu 0.27%\n",
            "iter 1090: loss 4.9703, time 971.99ms, mfu 0.28%\n",
            "iter 1100: loss 4.5206, time 975.68ms, mfu 0.28%\n",
            "iter 1110: loss 4.5361, time 973.42ms, mfu 0.28%\n",
            "iter 1120: loss 4.4706, time 971.37ms, mfu 0.28%\n",
            "iter 1130: loss 4.6965, time 973.05ms, mfu 0.28%\n",
            "iter 1140: loss 4.7673, time 971.56ms, mfu 0.28%\n",
            "iter 1150: loss 4.7339, time 972.68ms, mfu 0.28%\n",
            "iter 1160: loss 4.6169, time 968.87ms, mfu 0.28%\n",
            "iter 1170: loss 4.4460, time 967.53ms, mfu 0.28%\n",
            "iter 1180: loss 4.6236, time 974.82ms, mfu 0.28%\n",
            "iter 1190: loss 4.2463, time 971.31ms, mfu 0.28%\n",
            "step 1200: train loss 4.4745, val loss 5.3131\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 1200: loss 4.4703, time 1439.38ms, mfu 0.27%\n",
            "iter 1210: loss 4.5629, time 965.08ms, mfu 0.27%\n",
            "iter 1220: loss 4.3582, time 975.74ms, mfu 0.27%\n",
            "iter 1230: loss 4.2978, time 971.24ms, mfu 0.27%\n",
            "iter 1240: loss 4.5788, time 969.87ms, mfu 0.27%\n",
            "iter 1250: loss 4.5312, time 974.80ms, mfu 0.27%\n",
            "iter 1260: loss 4.3682, time 969.67ms, mfu 0.27%\n",
            "iter 1270: loss 4.4448, time 974.79ms, mfu 0.27%\n",
            "iter 1280: loss 4.2216, time 971.09ms, mfu 0.28%\n",
            "iter 1290: loss 4.4895, time 972.53ms, mfu 0.28%\n",
            "iter 1300: loss 4.5766, time 973.04ms, mfu 0.28%\n",
            "iter 1310: loss 4.3410, time 971.61ms, mfu 0.28%\n",
            "iter 1320: loss 4.5587, time 970.65ms, mfu 0.28%\n",
            "iter 1330: loss 4.3717, time 972.18ms, mfu 0.28%\n",
            "iter 1340: loss 4.2218, time 970.78ms, mfu 0.28%\n",
            "iter 1350: loss 4.5972, time 971.28ms, mfu 0.28%\n",
            "iter 1360: loss 4.1430, time 974.57ms, mfu 0.28%\n",
            "iter 1370: loss 4.2521, time 974.05ms, mfu 0.28%\n",
            "iter 1380: loss 4.1889, time 971.79ms, mfu 0.28%\n",
            "iter 1390: loss 4.0243, time 971.77ms, mfu 0.28%\n",
            "step 1400: train loss 4.2797, val loss 5.2390\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 1400: loss 4.0666, time 1432.78ms, mfu 0.27%\n",
            "iter 1410: loss 4.2086, time 972.35ms, mfu 0.27%\n",
            "iter 1420: loss 4.0953, time 969.89ms, mfu 0.27%\n",
            "iter 1430: loss 4.1091, time 973.41ms, mfu 0.27%\n",
            "iter 1440: loss 4.1397, time 971.08ms, mfu 0.27%\n",
            "iter 1450: loss 4.2468, time 972.77ms, mfu 0.27%\n",
            "iter 1460: loss 4.3786, time 973.88ms, mfu 0.27%\n",
            "iter 1470: loss 4.3073, time 968.76ms, mfu 0.27%\n",
            "iter 1480: loss 4.2154, time 973.97ms, mfu 0.28%\n",
            "iter 1490: loss 4.0115, time 971.77ms, mfu 0.28%\n",
            "iter 1500: loss 4.1226, time 966.72ms, mfu 0.28%\n",
            "iter 1510: loss 4.1224, time 975.17ms, mfu 0.28%\n",
            "iter 1520: loss 3.8574, time 966.65ms, mfu 0.28%\n",
            "iter 1530: loss 4.1369, time 975.02ms, mfu 0.28%\n",
            "iter 1540: loss 4.0719, time 970.52ms, mfu 0.28%\n",
            "iter 1550: loss 3.9172, time 977.80ms, mfu 0.28%\n",
            "iter 1560: loss 4.1057, time 972.01ms, mfu 0.28%\n",
            "iter 1570: loss 4.0362, time 971.14ms, mfu 0.28%\n",
            "iter 1580: loss 4.0080, time 970.86ms, mfu 0.28%\n",
            "iter 1590: loss 3.9641, time 971.77ms, mfu 0.28%\n",
            "step 1600: train loss 4.0204, val loss 5.5102\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 1600: loss 3.6595, time 1444.51ms, mfu 0.27%\n",
            "iter 1610: loss 3.7091, time 972.74ms, mfu 0.27%\n",
            "iter 1620: loss 4.0776, time 972.17ms, mfu 0.27%\n",
            "iter 1630: loss 4.0015, time 974.19ms, mfu 0.27%\n",
            "iter 1640: loss 4.0549, time 972.81ms, mfu 0.27%\n",
            "iter 1650: loss 3.7108, time 970.51ms, mfu 0.27%\n",
            "iter 1660: loss 4.0703, time 969.70ms, mfu 0.27%\n",
            "iter 1670: loss 3.9911, time 971.40ms, mfu 0.27%\n",
            "iter 1680: loss 3.8512, time 974.85ms, mfu 0.28%\n",
            "iter 1690: loss 4.1481, time 972.13ms, mfu 0.28%\n",
            "iter 1700: loss 4.0080, time 979.79ms, mfu 0.28%\n",
            "iter 1710: loss 4.0122, time 970.46ms, mfu 0.28%\n",
            "iter 1720: loss 3.8889, time 970.18ms, mfu 0.28%\n",
            "iter 1730: loss 3.9326, time 964.72ms, mfu 0.28%\n",
            "iter 1740: loss 3.7205, time 977.36ms, mfu 0.28%\n",
            "iter 1750: loss 3.9666, time 971.75ms, mfu 0.28%\n",
            "iter 1760: loss 3.9010, time 971.87ms, mfu 0.28%\n",
            "iter 1770: loss 3.7395, time 969.55ms, mfu 0.28%\n",
            "iter 1780: loss 3.7700, time 965.59ms, mfu 0.28%\n",
            "iter 1790: loss 3.9994, time 974.16ms, mfu 0.28%\n",
            "step 1800: train loss 3.8256, val loss 5.6002\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 1800: loss 4.0762, time 1443.96ms, mfu 0.27%\n",
            "iter 1810: loss 3.7422, time 971.09ms, mfu 0.27%\n",
            "iter 1820: loss 3.6628, time 974.24ms, mfu 0.27%\n",
            "iter 1830: loss 3.9366, time 970.45ms, mfu 0.27%\n",
            "iter 1840: loss 3.8605, time 974.24ms, mfu 0.27%\n",
            "iter 1850: loss 3.8716, time 974.66ms, mfu 0.27%\n",
            "iter 1860: loss 3.4737, time 971.29ms, mfu 0.27%\n",
            "iter 1870: loss 3.6457, time 972.24ms, mfu 0.27%\n",
            "iter 1880: loss 3.7584, time 968.76ms, mfu 0.28%\n",
            "iter 1890: loss 3.5878, time 973.48ms, mfu 0.28%\n",
            "iter 1900: loss 3.8554, time 968.03ms, mfu 0.28%\n",
            "iter 1910: loss 3.7350, time 976.05ms, mfu 0.28%\n",
            "iter 1920: loss 3.6870, time 972.91ms, mfu 0.28%\n",
            "iter 1930: loss 3.4979, time 972.41ms, mfu 0.28%\n",
            "iter 1940: loss 3.5463, time 969.23ms, mfu 0.28%\n",
            "iter 1950: loss 3.7702, time 970.74ms, mfu 0.28%\n",
            "iter 1960: loss 3.7884, time 972.31ms, mfu 0.28%\n",
            "iter 1970: loss 3.7770, time 974.19ms, mfu 0.28%\n",
            "iter 1980: loss 3.6688, time 966.69ms, mfu 0.28%\n",
            "iter 1990: loss 3.5781, time 969.60ms, mfu 0.28%\n",
            "step 2000: train loss 3.5992, val loss 5.6381\n",
            "saving checkpoint to out-tiny-shakespeare\n",
            "iter 2000: loss 3.7500, time 1444.53ms, mfu 0.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HjHyymd8qND",
        "outputId": "04bd3f3e-60d9-4260-e6e5-d069262cad69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r data/shakespeare /content/drive/MyDrive/nanoGPT_data/\n",
        "!cp -r out-tiny-shakespeare /content/drive/MyDrive/nanoGPT_models/"
      ],
      "metadata": {
        "id": "Rgj2zAYr862l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}