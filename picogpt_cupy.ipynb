{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "picogpt-cupy.ipynb",
      "authorship_tag": "ABX9TyP06dL4UnUZIJr2kUS56ruT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/durfred/my-first-binder/blob/main/picogpt_cupy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0qzTtyDd-Dd"
      },
      "outputs": [],
      "source": [
        "# 确保你使用的是GPU运行时 (Runtime -> Change runtime type -> GPU)\n",
        "# 安装所有需要的库\n",
        "!pip install torch transformers numpy cupy-cuda12x requests tqdm -q # -q 静默安装\n",
        "\n",
        "# 导入所有需要的模块\n",
        "import cupy as np      # CuPy 将作为 np\n",
        "import numpy as np_cpu # NumPy 将作为 np_cpu，以避免与 CuPy 混淆\n",
        "import torch           # 用于 Hugging Face 模型\n",
        "import os              # 用于文件操作\n",
        "from tqdm.autonotebook import tqdm # 进度条显示\n",
        "from transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config # Hugging Face 相关\n",
        "\n",
        "print(\"所有库安装完成。\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- picoGPT 核心函数 (已修改为 CuPy 并修复了所有已知问题) ---\n",
        "\n",
        "def gpt2(inputs, wte, wpe, blocks, ln_f_g, ln_f_b, n_head):\n",
        "    \"\"\"\n",
        "    GPT-2 模型的前向传播。所有输入权重都必须是 CuPy 数组。\n",
        "    inputs: (n_seq) - 当前输入token的序列ID\n",
        "    wte: (n_vocab, n_embd) - token嵌入权重\n",
        "    wpe: (n_seq_max, n_embd) - 位置嵌入权重 (n_seq_max 是 block_size)\n",
        "    blocks: 包含所有Transformer块参数的列表\n",
        "    ln_f_g, ln_f_b: 最终层归一化的增益和偏置\n",
        "    n_head: 注意力头的数量\n",
        "    \"\"\"\n",
        "    n_seq = inputs.shape[0] # 当前输入序列的长度\n",
        "    n_embd = wte.shape[1]   # 嵌入维度\n",
        "\n",
        "    # 1. token + positional embeddings (令牌嵌入 + 位置嵌入)\n",
        "    # wte[inputs] 进行token查找，wpe[:n_seq] 获取当前序列长度对应的位置嵌入\n",
        "    x = wte[inputs] + wpe[:n_seq]\n",
        "\n",
        "    # 2. Transformer blocks (Transformer 块)\n",
        "    for i in range(len(blocks)):\n",
        "        # block函数需要解包blocks[i]中的所有参数\n",
        "        x = block(x, *blocks[i], n_head)\n",
        "\n",
        "    # 3. Final layer normalization (最终层归一化)\n",
        "    x = layernorm(x, ln_f_g, ln_f_b)\n",
        "\n",
        "    # 4. Linear layer (output logits) (线性层，输出logits)\n",
        "    # 将结果乘以 token 嵌入矩阵的转置，得到每个词的概率分数\n",
        "    logits = x @ wte.T\n",
        "\n",
        "    return logits\n",
        "\n",
        "def block(x, ln_1_g, ln_1_b, attn_c_attn_w, attn_c_attn_b, attn_c_proj_w, attn_c_proj_b, mlp_c_fc_w, mlp_c_fc_b, mlp_c_proj_w, mlp_c_proj_b, n_head):\n",
        "    \"\"\"一个 Transformer 块的前向传播\"\"\"\n",
        "    # 1. 第一个层归一化和注意力机制\n",
        "    attn_output = attn(layernorm(x, ln_1_g, ln_1_b), attn_c_attn_w, attn_c_attn_b, attn_c_proj_w, attn_c_proj_b, n_head)\n",
        "    x = x + attn_output # 残差连接\n",
        "\n",
        "    # 2. 第二个层归一化和MLP (多层感知机)\n",
        "    # 注意：这里的layernorm再次使用ln_1_g,ln_1_b，与原始picoGPT代码一致\n",
        "    mlp_output = mlp(layernorm(x, ln_1_g, ln_1_b), mlp_c_fc_w, mlp_c_fc_b, mlp_c_proj_w, mlp_c_proj_b)\n",
        "    x = x + mlp_output # 残差连接\n",
        "\n",
        "    return x\n",
        "\n",
        "def layernorm(x, g, b):\n",
        "    \"\"\"层归一化\"\"\"\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    variance = np.mean(np.square(x - mean), axis=-1, keepdims=True)\n",
        "    # 加上一个很小的epsilon以防止除以零\n",
        "    x = (x - mean) * (1 / np.sqrt(variance + 1e-5)) * g + b\n",
        "    return x\n",
        "\n",
        "def attn(x, c_attn_w, c_attn_b, c_proj_w, c_proj_b, n_head):\n",
        "    \"\"\"自注意力机制\"\"\"\n",
        "    n_seq, n_embd = x.shape\n",
        "    n_state = n_embd // n_head # 每个注意力头的维度\n",
        "\n",
        "    # --- 请在此处添加以下调试打印语句 ---\n",
        "    print(f\"DEBUG (attn): 输入 x 的形状: {x.shape}\")\n",
        "    print(f\"DEBUG (attn): c_attn_w 的形状: {c_attn_w.shape}\")\n",
        "    print(f\"DEBUG (attn): n_embd (从 x 推断): {n_embd}\")\n",
        "    print(f\"DEBUG (attn): n_head: {n_head}\")\n",
        "    # --- 调试打印语句结束 ---\n",
        "\n",
        "    # 1. 查询 (Q), 键 (K), 值 (V) 投影\n",
        "    x = x @ c_attn_w + c_attn_b\n",
        "    q, k, v = np.split(x, 3, axis=-1) # 将输出沿最后一个维度分成3份 (Q, K, V)\n",
        "\n",
        "    # 2. 分割成多个注意力头\n",
        "    # (n_seq, n_head, n_state) -> transpose (n_head, n_seq, n_state)\n",
        "    q = q.reshape(n_seq, n_head, n_state).transpose(1, 0, 2)\n",
        "    k = k.reshape(n_seq, n_head, n_state).transpose(1, 0, 2)\n",
        "    v = v.reshape(n_seq, n_head, n_state).transpose(1, 0, 2)\n",
        "\n",
        "    # 3. 计算注意力分数 (Q @ K^T) / sqrt(d_k) + 因果掩码 + Softmax\n",
        "    scores = q @ k.transpose(0, 2, 1) / np.sqrt(n_state)\n",
        "\n",
        "    # 创建因果掩码 (上三角矩阵为负无穷，防止看到未来信息)\n",
        "    causal_mask = (1 - np.tri(n_seq, dtype=np.float32)) * -1e10\n",
        "    scores = scores + causal_mask # 应用掩码\n",
        "\n",
        "    # Softmax 归一化\n",
        "    scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    scores = scores / np.sum(scores, axis=-1, keepdims=True)\n",
        "\n",
        "    # 4. 加权求和 (scores @ V)\n",
        "    x = scores @ v\n",
        "    # 拼接多个注意力头的输出\n",
        "    x = x.transpose(1, 0, 2).reshape(n_seq, n_embd)\n",
        "\n",
        "    # 5. 线性投影\n",
        "    x = x @ c_proj_w + c_proj_b\n",
        "\n",
        "    return x\n",
        "\n",
        "def mlp(x, c_fc_w, c_fc_b, c_proj_w, c_proj_b):\n",
        "    \"\"\"多层感知机 (MLP)\"\"\"\n",
        "    # 1. 线性层 1\n",
        "    x = x @ c_fc_w + c_fc_b\n",
        "\n",
        "    # 2. GELU 激活函数\n",
        "    # 使用近似公式 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n",
        "    x = 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
        "\n",
        "    # 3. 线性层 2\n",
        "    x = x @ c_proj_w + c_proj_b\n",
        "\n",
        "    return x\n",
        "\n",
        "print(\"picoGPT核心函数定义完成。\")"
      ],
      "metadata": {
        "id": "67_Qa1nHe2E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face GPT-2 权重转换脚本 (已修正转置问题) ---\n",
        "\n",
        "def convert_hf_to_pico_weights(model_name=\"gpt2\", output_filename=\"model.npz\"):\n",
        "    \"\"\"\n",
        "    从 Hugging Face 加载 GPT-2 权重，并将其转换为 picoGPT 的 .npz 格式。\n",
        "    已修正 Conv1D 权重的转置问题。\n",
        "    \"\"\"\n",
        "    print(f\"1. 加载 Hugging Face GPT-2 模型: {model_name}...\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    pico_weights = {}\n",
        "\n",
        "    print(\"2. 转换 Embedding (词嵌入) 和最终层归一化权重...\")\n",
        "    # Token Embedding\n",
        "    pico_weights['wte'] = state_dict['transformer.wte.weight'].numpy()\n",
        "    # Positional Embedding\n",
        "    pico_weights['wpe'] = state_dict['transformer.wpe.weight'].numpy()\n",
        "\n",
        "    # Final Layer Normalization (最终层归一化)\n",
        "    pico_weights['ln_f_g'] = state_dict['transformer.ln_f.weight'].numpy()\n",
        "    pico_weights['ln_f_b'] = state_dict['transformer.ln_f.bias'].numpy()\n",
        "\n",
        "    print(f\"3. 转换 {model.config.n_layer} 个 Transformer 块的权重...\")\n",
        "    for i in range(model.config.n_layer):\n",
        "        prefix = f'transformer.h.{i}.'\n",
        "        block_key = f'h{i}'\n",
        "\n",
        "        # Layer Normalization 1\n",
        "        pico_weights[f'{block_key}_ln_1_g'] = state_dict[prefix + 'ln_1.weight'].numpy()\n",
        "        pico_weights[f'{block_key}_ln_1_b'] = state_dict[prefix + 'ln_1.bias'].numpy()\n",
        "\n",
        "        # Attention weights (注意力权重) - 移除 .T\n",
        "        # Hugging Face Conv1D 权重已经是 (in_features, out_features) 形状，不需要转置\n",
        "        pico_weights[f'{block_key}_attn_c_attn_w'] = state_dict[prefix + 'attn.c_attn.weight'].numpy() # REMOVED .T\n",
        "        pico_weights[f'{block_key}_attn_c_attn_b'] = state_dict[prefix + 'attn.c_attn.bias'].numpy()\n",
        "\n",
        "        pico_weights[f'{block_key}_attn_c_proj_w'] = state_dict[prefix + 'attn.c_proj.weight'].numpy() # REMOVED .T\n",
        "        pico_weights[f'{block_key}_attn_c_proj_b'] = state_dict[prefix + 'attn.c_proj.bias'].numpy()\n",
        "\n",
        "        # MLP (前馈网络) 权重 - 移除 .T\n",
        "        pico_weights[f'{block_key}_mlp_c_fc_w'] = state_dict[prefix + 'mlp.c_fc.weight'].numpy() # REMOVED .T\n",
        "        pico_weights[f'{block_key}_mlp_c_fc_b'] = state_dict[prefix + 'mlp.c_fc.bias'].numpy()\n",
        "\n",
        "        pico_weights[f'{block_key}_mlp_c_proj_w'] = state_dict[prefix + 'mlp.c_proj.weight'].numpy() # REMOVED .T\n",
        "        pico_weights[f'{block_key}_mlp_c_proj_b'] = state_dict[prefix + 'mlp.c_proj.bias'].numpy()\n",
        "\n",
        "    # 将转换后的权重保存为 .npz 格式\n",
        "    np_cpu.savez(output_filename, pico_weights)\n",
        "    print(f\"4. 转换并保存权重到 {output_filename}. 文件大小: {os.path.getsize(output_filename)/(1024*1024):.2f} MB\")\n",
        "\n",
        "    return output_filename\n",
        "\n",
        "# --- 执行权重转换 ---\n",
        "!rm -f model.npz # 移除旧的 model.npz 文件\n",
        "print(\"前一个 model.npz 文件已移除。\")\n",
        "\n",
        "# 重新执行转换过程，生成新的 model.npz 文件\n",
        "generated_npz_file = convert_hf_to_pico_weights(model_name=\"gpt2\", output_filename=\"model.npz\")\n",
        "\n",
        "print(f\"\\n成功创建了 {generated_npz_file}. 文件已准备好加载。\")"
      ],
      "metadata": {
        "id": "03eKyUJoe5aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 加载 picoGPT 权重 (已修改为 CuPy 并兼容扁平化结构) ---\n",
        "\n",
        "def load_gpt2_weights_cupy(filename=\"model.npz\"):\n",
        "    \"\"\"\n",
        "    加载 GPT-2 small 模型权重，并将其加载为 CuPy 数组。\n",
        "    兼容由 `convert_hf_to_pico_weights` 函数生成的扁平化 .npz 文件。\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"错误: {filename} 未找到。请确保 Cell 3 成功运行并生成了文件！\")\n",
        "        return None # 或者抛出异常\n",
        "\n",
        "    print(f\"加载 {filename} (由转换脚本生成)...\")\n",
        "    # 使用 NumPy (CPU) 加载 .npz 文件\n",
        "    params_npz_file = np_cpu.load(filename, allow_pickle=True)\n",
        "\n",
        "    # 获取包含所有权重的字典 (它被保存为 'arr_0' 键的值)\n",
        "    params_dict = params_npz_file['arr_0'].item()\n",
        "\n",
        "    # 提取GPT-2 small的固定配置参数\n",
        "    n_vocab = 50257 # 词汇表大小\n",
        "    n_embd = 768    # 嵌入维度\n",
        "    n_head = 12     # 注意力头数量\n",
        "    n_block = 12    # Transformer 块数量\n",
        "    block_size = 1024 # 模型上下文窗口大小 (最大序列长度)\n",
        "\n",
        "    # 映射权重到picoGPT的参数，并将其转换为 CuPy 数组 (加载到 GPU 内存)\n",
        "    wte = np.asarray(params_dict['wte'])\n",
        "    wpe = np.asarray(params_dict['wpe'])\n",
        "    ln_f_g = np.asarray(params_dict['ln_f_g'])\n",
        "    ln_f_b = np.asarray(params_dict['ln_f_b'])\n",
        "\n",
        "    blocks = []\n",
        "    for i in range(n_block):\n",
        "        block_key = f'h{i}' # 构造扁平化键名 (例如 'h0', 'h1'...)\n",
        "\n",
        "        # 从扁平化字典中直接获取权重，并转换为 CuPy 数组\n",
        "        ln_1_g = np.asarray(params_dict[f'{block_key}_ln_1_g'])\n",
        "        ln_1_b = np.asarray(params_dict[f'{block_key}_ln_1_b'])\n",
        "        attn_c_attn_w = np.asarray(params_dict[f'{block_key}_attn_c_attn_w'])\n",
        "        attn_c_attn_b = np.asarray(params_dict[f'{block_key}_attn_c_attn_b'])\n",
        "        attn_c_proj_w = np.asarray(params_dict[f'{block_key}_attn_c_proj_w'])\n",
        "        attn_c_proj_b = np.asarray(params_dict[f'{block_key}_attn_c_proj_b'])\n",
        "        mlp_c_fc_w = np.asarray(params_dict[f'{block_key}_mlp_c_fc_w'])\n",
        "        mlp_c_fc_b = np.asarray(params_dict[f'{block_key}_mlp_c_fc_b'])\n",
        "        mlp_c_proj_w = np.asarray(params_dict[f'{block_key}_mlp_c_proj_w'])\n",
        "        mlp_c_proj_b = np.asarray(params_dict[f'{block_key}_mlp_c_proj_b'])\n",
        "\n",
        "        # 将当前块的所有参数打包成一个列表，添加到 blocks 列表中\n",
        "        blocks.append([ln_1_g, ln_1_b, attn_c_attn_w, attn_c_attn_b, attn_c_proj_w, attn_c_proj_b,\n",
        "                       mlp_c_fc_w, mlp_c_fc_b, mlp_c_proj_w, mlp_c_proj_b])\n",
        "\n",
        "    print(\"GPT-2 small 权重已成功加载到 CuPy (GPU 内存)。\")\n",
        "    return wte, wpe, blocks, ln_f_g, ln_f_b, n_head, n_vocab, block_size\n",
        "\n",
        "# --- 执行加载 ---\n",
        "# 调用函数加载权重，并将返回的参数解包到对应变量\n",
        "wte, wpe, blocks, ln_f_g, ln_f_b, n_head, n_vocab, block_size = load_gpt2_weights_cupy()"
      ],
      "metadata": {
        "id": "22g2BYb0fAND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 文本生成函数 (CuPy 版本) ---\n",
        "\n",
        "def generate_text_cupy(prompt, model_params, tokenizer, max_tokens=50, temperature=1.0, top_k=0):\n",
        "    \"\"\"\n",
        "    使用 CuPy 加载的 GPT-2 模型生成文本。\n",
        "    prompt: 初始提示文本 (字符串)\n",
        "    model_params: 包含所有模型参数的元组 (wte, wpe, blocks, ...)\n",
        "    tokenizer: Hugging Face 分词器\n",
        "    max_tokens: 最大生成token数量\n",
        "    temperature: 采样温度 (越高越随机)\n",
        "    top_k: Top-K 采样参数 (只考虑概率最高的K个token)\n",
        "    \"\"\"\n",
        "    wte, wpe, blocks, ln_f_g, ln_f_b, n_head, n_vocab, block_size = model_params\n",
        "\n",
        "    # 1. 编码提示 (分词器在 CPU 上工作)\n",
        "    # return_tensors=\"np\" 返回 NumPy 数组\n",
        "    input_ids_np = tokenizer.encode(prompt, return_tensors=\"np\")[0].astype(np_cpu.int32)\n",
        "\n",
        "    # 确保输入长度不超过模型的最大上下文窗口\n",
        "    if len(input_ids_np) > block_size:\n",
        "        input_ids_np = input_ids_np[-block_size:] # 截断为最后 block_size 个 token\n",
        "\n",
        "    # 关键：将初始输入 token ID 从 NumPy (CPU) 转移到 CuPy (GPU)\n",
        "    input_ids_gpu = np.asarray(input_ids_np)\n",
        "\n",
        "    # generated_ids 列表在 CPU 上维护，因为 tokenizer.decode 需要 NumPy 数组\n",
        "    generated_ids = list(input_ids_np)\n",
        "\n",
        "    print(f\"正在生成 {max_tokens} 个 token (temperature={temperature}, top_k={top_k})...\")\n",
        "    print(f\"提示: {prompt}\")\n",
        "\n",
        "    for _ in tqdm(range(max_tokens), desc=\"生成中\"):\n",
        "        # 2. 准备当前输入：取当前生成序列的最后 `block_size` 个 token\n",
        "        # 每次迭代，将当前生成的序列 (或其截断部分) 从 CPU 移到 GPU\n",
        "        current_input_gpu = np.asarray(np_cpu.array(generated_ids[-block_size:]))\n",
        "\n",
        "        # 3. 运行 picoGPT 模型前向传播 (在 GPU 上执行)\n",
        "        logits_gpu = gpt2(current_input_gpu, wte, wpe, blocks, ln_f_g, ln_f_b, n_head)\n",
        "\n",
        "        # 4. 获取最后一个 token 的 logits (预测下一个词的概率分数)\n",
        "        next_token_logits_gpu = logits_gpu[-1, :]\n",
        "\n",
        "        # 5. 应用 temperature (温度) 和 Top-K 采样\n",
        "        if temperature != 0.0:\n",
        "            next_token_logits_gpu = next_token_logits_gpu / temperature\n",
        "\n",
        "        if top_k > 0:\n",
        "            # 对于 Top-K 采样，需要将 logits 移回 CPU 进行排序和过滤\n",
        "            next_token_logits_cpu = next_token_logits_gpu.get() # .get() 将 CuPy 数组复制回 NumPy (CPU)\n",
        "            # 将非 Top-K 的 logits 设置为负无穷，使其在 softmax 后概率为0\n",
        "            indices_to_remove = next_token_logits_cpu < np_cpu.sort(next_token_logits_cpu)[-top_k]\n",
        "            next_token_logits_cpu[indices_to_remove] = -float('Inf')\n",
        "            # 重新将其转换为 CuPy 数组 (虽然这里直接采样了，但好的习惯)\n",
        "            next_token_logits_gpu = np.asarray(next_token_logits_cpu)\n",
        "\n",
        "\n",
        "        # 6. Softmax 转换为概率 (仍在 GPU 上)\n",
        "        # 减去最大值以提高数值稳定性\n",
        "        probabilities_gpu = np.exp(next_token_logits_gpu - np.max(next_token_logits_gpu))\n",
        "        probabilities_gpu = probabilities_gpu / np.sum(probabilities_gpu)\n",
        "\n",
        "        # 7. 采样下一个 token ID\n",
        "        # 将概率数组从 GPU 移回 CPU，因为 np_cpu.random.choice 需要 NumPy 数组\n",
        "        next_token_id = np_cpu.random.choice(len(probabilities_gpu), p=probabilities_gpu.get())\n",
        "\n",
        "        # 8. 将新生成的 token ID 添加到已生成序列中\n",
        "        generated_ids.append(int(next_token_id)) # 确保是 Python int 类型\n",
        "\n",
        "    # 9. 解码整个生成序列 (分词器在 CPU 上解码)\n",
        "    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    print(\"\\n--- 完整生成文本 ---\")\n",
        "    print(full_generated_text)\n",
        "    return full_generated_text\n",
        "\n",
        "print(\"文本生成函数定义完成。\")"
      ],
      "metadata": {
        "id": "g9SYzdDzfFFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 实例化分词器并运行文本生成 ---\n",
        "\n",
        "# 确保 tokenizer 已经导入 (在 Cell 1 中)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# 将所有加载的 CuPy 权重和参数打包成一个元组，方便传递给生成函数\n",
        "# 这些变量 (wte, wpe, blocks, ...) 应该已经在 Cell 4 中被成功加载并赋值\n",
        "model_parameters_cupy = (wte, wpe, blocks, ln_f_g, ln_f_b, n_head, n_vocab, block_size)\n",
        "\n",
        "# 定义你的初始提示文本\n",
        "prompt_cupy = \"The quick brown fox jumps over the lazy\"\n",
        "\n",
        "# 开始生成文本！\n",
        "print(\"\\n开始文本生成...\")\n",
        "generated_output_cupy = generate_text_cupy(prompt_cupy, model_parameters_cupy, tokenizer, max_tokens=50, temperature=0.7, top_k=50)\n",
        "\n",
        "print(\"\\n文本生成完成。\")"
      ],
      "metadata": {
        "id": "Ib_6LVYUfIKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}